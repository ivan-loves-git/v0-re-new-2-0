---
title: Lessons Learned
purpose: Document key learnings from campaigns, experiments, and strategic pivots
version: 1.0
created: 2024-11-01
author: Team
status: canonical
last_reviewed: 2024-11-01
source: Campaign #2 Debrief + Meeting Oct 24, 2024
---

# Lessons Learned

## Campaign #2 Learnings (October-November 2024)

### Major Outcome: Selection Model Doesn't Scale

**Date:** October 24, 2024 meeting + Nov 2024 debrief
**Source:** Campaign #2 retrospective (Amelie + Bertrand)

**Key insight:**
"In the end we are not dealing with candidates actually because we do not provide any kind of job opportunities... we are dealing with repreneurs and these repreneurs can be either products that we may sell to a partner or customers of our different services."

**What we learned:**

1. **Campaign as marketing tool, not selection process:**
   - Campaigns work for buzz and LinkedIn visibility (1-2x per year)
   - Scoring and ranking of candidates is "not very interesting" for long-term value
   - "We are not selecting repreneurs, we are identifying and proposing solutions"
   - Campaign provides market validation and lead generation, not competitive selection

2. **Lead de Cadrage is the real asset:**
   - Structured profiling document is "the magic key" (Bertrand)
   - No repreneur enters Re-New ecosystem without completing it
   - Unstructured interviews are less valuable than structured data collection
   - Forms with structured data > PDF documents for matching and analytics
   - Quality filter: Completing it demonstrates project seriousness

3. **Repreneur value varies by journey stage:**
   - Not all repreneurs are equal; value depends on "where they stand on their entrepreneurship journey"
   - Stages identified: Super early / Quite early / Advanced / Super advanced
   - Different stages require different offers and monetization approaches
   - Service design must match repreneur maturity level

4. **Interviews shifted from HR to consultative:**
   - Campaign #1: "Real HR, quite boring interviews"
   - Campaign #2: "Challenged by the candidates... more of a conversation"
   - Repreneurs challenge Re-New on what can be provided = reveals product gaps
   - This consultative dynamic "enlightened us on missing parts of what we could propose"
   - Signals shift from gatekeeper to service provider mindset

5. **Scoring/matching/resumes are not differentiators:**
   - "Everything that we did previously like the scoring, the matching, the resumes... in the end we don't give a shit" (Amelie)
   - Quick profiling useful but "the most important is what they said from day one: the Lead de Cadrage"
   - Traditional recruitment metrics don't apply to entrepreneurship ecosystem

---

### Operational Learnings: Tools & Scale

**Problem identified:**
- Bertrand managing 70+ active repreneur relationships manually
- Fletcher (ATS) inadequate: "We cannot keep on working like this... this is not okay"
- Spreadsheets insufficient for tracking actions, offers, partner introductions
- Interview recordings exist but underutilized (stored in Notion, not systematically analyzed)

**Bertrand's pipeline (Nov 2024):**
- 8 Campaign #2 follow-ups
- 4 Campaign #2 finalists
- 10 from previous campaign still active
- 6 very promising (meetings scheduled)
- 10+ from recent outreach
- 35+ in pipeline to meet
- **Total: ~70 relationships to manage**

**Decision:**
- Shift from ATS (Applicant Tracking System) to CRM (Customer Relationship Management)
- Flatchr phased out for relationship management
- Airtable CRM under evaluation ("we have Airtable as a CRM, I have an account" - Bertrand)

**Lesson:** Selection tools ` Relationship management tools. Re-New outgrew HR tech stack.

**Scaling threshold identified:** ~35 active relationships is breaking point for manual management in spreadsheets.

---

### Monetization Learnings

**Partner-side economics validated (Entrepreneurs & Finance example):**
- M&A firm receives 10% commission on ï¿½3M deal = ï¿½300K
- Re-New receives ï¿½25K = ~1% of deal value
- **Problem:** "Maximum will be 20 [deals] per year... not a long-term enough way of monetization" (Amelie)
- Long sales cycles (deal closings take months)
- Limited scalability compared to repreneur-side potential

**Repreneur-side monetization needs testing:**
- Bertrand spending "enormous quantity of hours" coaching repreneurs for free
- "At some point this should be monetized" (Amelie)
- Proposed model: Small fees for coaching/introductions (ï¿½100 for 2 months example)
- Three service layers identified:
  - **Layer 1:** Selection, deal flow access, network (currently free)
  - **Layer 2:** Operating partner support post-acquisition
  - **Layer 3:** Investment vehicle access

**Dual monetization strategy:**
- "We want to try both [partner and repreneur monetization] and see which one produces something" (Amelie)
- Recognition that one may cannibalize the other
- Timeline: 3-6 months testing, then decide
- **Lesson:** Experimentation phase with clear decision timeline is critical

**Economic reality:**
- Zero revenue to date (Nov 2024)
- Can't sustain free services indefinitely
- Need willingness-to-pay validation from at least one side
- Cash flow urgency driving dual testing approach

---

### Strategic Clarity: What Re-New Is NOT

**Not a hiring platform:**
- "We do not provide any kind of job opportunities"
- "Candidates" terminology is misleading and must be eliminated
- Shift to "repreneur" throughout all communications

**Not a pure matchmaking platform (open question):**
- Debate over building "Dealmaker-style" dashboard
- Value may be in curation and relationships, not self-service browsing
- **Ivan's insight:** "The complexity is to get the data organized... for 120 deals, they can scroll it in 50 minutes, it's not that much of a value"
- Platform vs. boutique service model still being evaluated

**Not a one-time accelerator:**
- Repreneurs need ongoing support across journey stages
- Post-acquisition support (Operating Partner program) is part of value proposition
- Investment vehicle is third layer of engagement
- **Lesson:** Re-New is a multi-year relationship platform, not a cohort-based program

**Not a selection competition:**
- Early framing as "campaign with winners" was misleading
- No winners/losers in entrepreneurship support ecosystem
- Everyone at different stages, all need different support

---

### Data & Analytics Needs

**What's missing:**
- Systematic analytics on Campaign #2 funnel (who was filtered out, at what stage, why)
- Quantified segmentation of repreneurs by journey stage
- Attribution of deal flow sources (which M&A partners provide quality vs. quantity)
- Conversion metrics from Lead de Cadrage ï¿½ Partner introduction ï¿½ Deal closed
- Cost per repreneur (acquisition + service delivery)

**Action items identified:**
- Nacho to work on Campaign #2 analytics
- Use interview recordings to extract hidden patterns (Ivan's AI suggestion)
- Frame offers based on data patterns, not gut feel
- Build CRM around measurable stages and outcomes

**Lesson:** "We need the data and information... to design the offers" (Amelie)

**Ivan's offer:**
"With recent technologies I could extract so many hidden layers [from interview transcripts] at scale"
- But requires Re-New to frame what insights matter
- Can't just throw AI at unstructured data without hypothesis

---

### Validation in Progress

**What Campaign #2 confirmed:**
-  Market need exists (repreneurs actively seeking support)
-  Repreneur demand across journey stages (not just one profile type)
-  Partner network is accessible and engaged (deal flow coming in)
-  Lead de Cadrage creates strategic differentiation (structured profiling works)
-  Consultative model works (repreneurs engage deeply, challenge constructively)

**What we're still testing:**
- ï¿½ Willingness-to-pay from partners (will M&A firms pay commissions?)
- ï¿½ Willingness-to-pay from repreneurs (will they buy coaching/services?)
- ï¿½ Unit economics (cost to serve vs. revenue per repreneur)
- ï¿½ Scale potential with CRM vs. manual relationship management
- ï¿½ Platform strategy (aggregation vs. curation)
- ï¿½ Optimal repreneur-to-team ratio (how many can Bertrand manage?)

---

## Meta-Learning: How to Learn

**Meeting recordings as strategic asset:**
- Bertrand records all repreneur interviews in Notion with transcripts
- Transcripts available but not systematically analyzed
- Ivan: "With recent technologies I could extract so many hidden layers and make them available at scale"
- **Lesson:** Raw data (recordings, transcripts) is valuable but requires structured analysis to extract insights
- Need to define hypotheses before mining data ("frame what in the end is interesting")

**Framing before execution:**
- Ivan's challenge: "We need the framing... what in the end is interesting"
- Can't just throw AI at interviews; need to know what questions to ask
- **Lesson:** Define hypotheses before mining data, or risk drowning in noise

**Testing before building:**
- Ivan's advice on platform: "Fake it until you make it... press this button and give me the data for your deals"
- Test willingness before investing in technology
- Manual validation > premature automation
- **Lesson:** Validate demand with scrappy manual processes before automating

**Example - CRM testing approach:**
1. Define 3-5 offer tiers manually (Google Doc)
2. Pitch to 5-10 repreneurs personally
3. Track: Did they pay? What did delivery cost?
4. Only build CRM automation after proving offers work
5. Don't confuse "better tools" with "business model validation"

---

## Strategic Inflection Points

### October-November 2024: The Great Pivot

**FROM:**
- Campaign as recruitment/selection process
- Free service hunting for big commission payoff later
- ATS mindset (accept/reject candidates)
- Unstructured manual processes
- "Candidates" terminology
- Focus on scoring and ranking

**TO:**
- Campaign as marketing tool (1-2x/year for lead gen)
- Monetizing repreneur services directly (testing)
- CRM mindset (nurture customer relationships)
- Structured data enabling automation (Lead de Cadrage)
- "Repreneurs" terminology (ecosystem participants)
- Focus on relationship management and service delivery

**Core Tension Identified:**
Team is torn between two strategic paths:
1. **Platform play:** Aggregate deals, become marketplace (ambitious, technically complex, partner buy-in uncertain)
2. **Boutique service:** Curated matching, coaching, white-glove support (proven demand, scalability concerns)

**Ivan's meta-recommendation:**
- Test both, but frame as time-bound experiment
- Recognize they may be mutually exclusive long-term
- Validate willingness to pay BEFORE building technology
- Focus on generating revenue from current 35 repreneurs with scrappy manual offers before automating

**Critical Success Factor:**
Defining specific, priced offers for each repreneur segment and testing in market. This will determine which business model wins.

---

## Lessons from Ivan's Challenges

### Challenge: Technology ` Business Model

**Pattern observed:**
Team often jumped to "we need technology solution" when real problem was "we need revenue validation"

**Examples:**
1. **CRM request:** "We need a CRM to manage repreneurs"
   - **Ivan's challenge:** "Take 5 candidates, package an offer manually, sell it. If they buy, THEN build CRM."
   - **Lesson:** CRM enables delivery but doesn't create willingness-to-pay

2. **Matching automation:** "We need AI to match repreneurs to deals"
   - **Ivan's challenge:** "For 120 deals, they can scroll in 50 minutes. Matching isn't the bottleneck."
   - **Lesson:** Real bottleneck is aggregating deal data, not algorithm

3. **Platform ambition:** "Should we build Dealmaker for France?"
   - **Ivan's challenge:** "Email 100 partners: 'we're building platform, submit your deals here.' See response rate BEFORE building."
   - **Lesson:** Validate supply-side willingness before building marketplace

**Core principle:**
"Technologically everything is feasible. It's all about cost-benefit." (Ivan)
- Technology should scale what's already working manually
- Don't build before validating core hypothesis
- Fake it, test it, measure it, THEN build it

---

### Challenge: Focus > Tools

**The Fletcher Conversation:**

**Team:** "We need better tools, Fletcher isn't working"
**Ivan:** "I don't see money coming from better tools. You have 5 candidatesgo sell them an offer. That creates money, not a CRM."

**Insight:**
- Better tools create efficiency, not revenue
- Revenue comes from value delivery and willingness-to-pay
- Don't confuse operational improvements with business model validation

**When tools matter:**
- AFTER proving offers work manually
- AFTER hitting scale where manual breaks (35-50 repreneurs for Bertrand)
- AFTER understanding exactly what to automate

**When tools don't matter:**
- BEFORE validating willingness-to-pay
- BEFORE defining specific service offerings
- BEFORE understanding unit economics

---

### Challenge: Platform Strategy Nuance

**Ivan's framework:**
"Platform strategy: Keep one side free if that brings more value on the other side. Don't try to squeeze money from everyone."

**Examples:**
- Uber: Riders free, drivers pay
- Airbnb: Guests mostly free, hosts pay
- LinkedIn: Users free, recruiters pay

**Re-New consideration:**
If keeping repreneurs free ï¿½ more/better repreneurs ï¿½ partners pay more ï¿½ total revenue may be higher than charging both sides

**Counterpoint (Amelie):**
"Bertrand's time is expensive. If repreneurs don't pay, are we undervaluing our service?"

**Resolution:**
Test both. Measure. Decide based on data, not ideology.
- 3-6 month experiment
- Track conversion rates, revenue, customer satisfaction
- Choose winner or continue both only if both validate

**Lesson:** Platform economics are counterintuitive. Volume on free side can create more value than charging everyone.

---

### Challenge: One Business Model Will Win

**Ivan's warning:**
"At some point, one is going to cannibalize the other unless made very strategically well."

**Why both streams may conflict:**
- If partners know repreneurs are paying Re-New, may reduce perceived value of "vetted candidates"
- If repreneurs know partners are paying Re-New for them, may question loyalties
- Team focus split between two sales motions (partners and repreneurs)
- Brand messaging confusion (who do we serve? who pays us?)

**How to manage:**
- **Segmentation:** Different value props (partners pay for vetting, repreneurs pay for coaching)
- **Time-boxing:** 3-6 month experiment, then choose
- **Clear metrics:** Which has better conversion? Unit economics? Scalability?
- **Willingness to kill:** If one fails, stop immediately and focus on winner

**Lesson:** Strategic clarity matters more than preserving optionality. Choose a path and commit.

---

## Recurring Patterns

### 1. Terminology Shapes Thinking

**"Candidates" ï¿½ "Repreneurs"**
- Using hiring language ("candidates," "ATS," "scoring") reinforced wrong mental model
- Took Campaign #2 to realize: "We're not hiring anyone!"
- Shifting to "repreneurs" clarifies Re-New's role: ecosystem support, not selection

**Lesson:** Language matters. Wrong words = wrong strategy.

---

### 2. What Gets Measured Gets Managed

**ATS metrics (wrong):**
- Applicants received
- Scoring distribution
- Selection rate
- Campaign cohort performance

**CRM metrics (right):**
- Repreneurs by lifecycle stage
- Services delivered per repreneur
- Revenue per repreneur (actual + pipeline)
- Partner commission deals
- Time invested vs. monetization (ROI per repreneur)

**Lesson:** Shifting metrics shifts focus from "how many selected" to "how much value created and captured."

---

### 3. Manual First, Automate Later

**Every time the pattern repeated:**
1. Team identifies problem
2. Team proposes technology solution
3. Ivan asks: "Have you done this manually to prove it works?"
4. Team admits: "Not yet"
5. Ivan recommends: "Test manually first, automate when it breaks"

**Examples:**
- CRM: Do manual outreach to 5-10 repreneurs first
- Matching: Use Google Sheet before building algorithm
- Platform: Email partners before building portal

**Lesson:** Manual processes force clarity on what value you're creating. Automation before validation = waste.

---

### 4. Willingness-to-Pay is the Only Truth

**Everything else is hypothesis:**
- "Partners will value vetted repreneurs" ï¿½ hypothesis until they pay
- "Repreneurs will pay for coaching" ï¿½ hypothesis until they pay
- "Platform aggregation creates value" ï¿½ hypothesis until both sides participate

**The only signal that matters:**
Did they open their wallet?

**Lesson:** Customer development interviews are useful. Signed contracts are truth.

---

## Open Questions (Still Learning)

### 1. Platform vs. Boutique?

**Platform model:**
- Aggregate deals from many M&A firms
- Provide dashboard for repreneurs to browse
- Scale through network effects
- Lower touch, higher volume

**Boutique model:**
- Curated matching by Bertrand's judgment
- High-touch relationship management
- Scale through team expansion
- Higher touch, lower volume

**What we need to learn:**
- Will M&A firms share deal data systematically?
- Do repreneurs value self-service or curation more?
- Can Re-New defend margins in boutique model as competitors emerge?
- Can Re-New aggregate enough deals to justify platform investment?

**Status:** Unresolved. Testing needed.

---

### 2. Geographic Expansion: When?

**Ivan's framing:**
"Should you prioritize depth (France) or breadth (multi-country)? Is the Italian opportunity a distraction or strategic advantage?"

**Considerations:**
- France: Deeper partner network, more deal flow, proven model
- Italy: New market, requires localization, founder connections exist
- Risk: Spreading too thin before France is working

**What we need to learn:**
- What's the right time to expand geographically?
- Should we hit revenue milestone in France first?
- Or does multi-country positioning help fundraising?

**Status:** Unresolved. France focus for now.

---

### 3. Optimal Repreneur-to-Team Ratio?

**Current state:**
- Bertrand managing ~70 repreneurs (manual, breaking)
- With CRM: Can he manage 100? 150?
- With team: What's the ratio? 50 repreneurs per relationship manager?

**What we need to learn:**
- How many repreneurs can one person manage well?
- Does it vary by repreneur stage (early needs more touch)?
- What tasks can be automated vs. must stay high-touch?

**Status:** Unresolved. Will learn as CRM scales.

---

## Next Learning Priorities

### Immediate (Nov-Dec 2024)
1. **Campaign #2 analytics** (Nacho): Quantify funnel, segmentation, drop-off points
2. **Offer design** (Team): Define 3-5 service tiers with pricing
3. **Willingness-to-pay tests** (Bertrand): Pitch offers to 5-10 repreneurs
4. **CRM setup** (Amelie): Migrate to Airtable CRM, test with active repreneurs

### Short-term (Q1 2025)
1. **Dual monetization results**: Which stream is working? Partners or repreneurs?
2. **Unit economics**: Cost per repreneur vs. revenue per repreneur
3. **Matching quality**: Are Lead de Cadrage-based matches relevant?
4. **Interview transcript mining**: Extract patterns across 20+ interviews

### Medium-term (Q2-Q3 2025)
1. **Platform decision**: Build deal aggregation or stay boutique?
2. **Geographic expansion**: Italy pilot or France depth?
3. **Team scaling**: Hire relationship managers or keep lean?
4. **Fundraising**: Do we need capital? How much? When?

---

## AI Recruitment Platform Learnings (Pre-Campaign #2)

### Background: The Platform That Wasn't Deployed

**Date:** Pre-October 2024
**Built by:** Ivan
**Status:** Completed but never deployed

**Context:** Ivan built comprehensive AI-powered recruitment platform to automate CV screening, reduce cost-per-candidate, and scale application processing. Platform featured 3-stage evaluation, 8 AI bots for scoring, automated personalization, and cost tracking.

**Why not deployed:** Business problem changed. Original assumption was high-volume screening challenge (many CVs, limited time). Reality became relationship depth challenge (fewer candidates, need deep ongoing support). Platform optimized for wrong bottleneck.

**Archive location:** [AI Platform Documentation](../../07_ARCHIVE/ai-recruitment-platform/)

---

### Lesson 1: Build for the Business You Have, Not the Business You Imagine

**What happened:**
Platform assumed Re-New would receive hundreds of candidate CVs and need fast, automated screening to manage volume efficiently. Built sophisticated 3-stage system with AI scoring, personalized emails, cost tracking.

**Reality discovered:**
- Application volumes decreased (not increased)
- Bottleneck shifted from "screen many fast" to "manage few deeply"
- Relationship management became constraint, not CV processing speed
- Re-New's value is ongoing support, not selection efficiency

**Lesson learned:**
Validate core assumptions BEFORE building. If assumption is "we'll have too many applicants," test with small pilot campaign first. Don't build infrastructure for imagined scale problems.

**Application to future:**
- Start with manual process for new initiatives
- Build only when manual process breaks under real load
- Question: "Is this the bottleneck TODAY, or the bottleneck we imagine for tomorrow?"

---

### Lesson 2: Two-Step Data Collection Maximizes Conversion

**Platform insight:** Don't ask for everything upfront.

**Approach:**
1. **Step 1 (low friction):** Just CV, minimal barrier to entry
2. **Step 2 (after credibility built):** Follow-up questionnaire via personalized email

**Rationale (Ivan's design philosophy):**
> "You will lose some traction if you ask everything at the beginning. This way you can get more conversion... you're building credibility because receiving an email after your CV sent saying you are good for this this and that reason, I think it's nice added value."

**Campaign #2 validation:**
This approach was retained in Campaign #2 with similar structure:
- Initial application via Flatchr (CV + basic info)
- Stage 2 questionnaire for qualified candidates
- Higher conversion vs. single long form

**Lesson learned:**
Staging data collection reduces friction AND builds relationship. First touch should be low-commitment; deeper questions come after demonstrating value.

**Application to future:**
- Lead de Cadrage (comprehensive project profile) should come AFTER initial engagement, not before
- CRM follow-up sequences should progressively request more detailed information
- Partner data collection: Start with basic deal info, request comprehensive data after proving value

---

### Lesson 3: Cost-Per-Unit Tracking Enables Strategic Decisions

**Platform feature:** Tracked cost-per-candidate at assessment stage.

**Methodology:**
- Each team member assigned hourly rate
- Time spent per assessment recorded (5 min skill test, 30 min interview, etc.)
- Real-time cost calculation: (Assessors Ã— Hourly Rate Ã— Time)
- Dashboard tracked daily and cumulative costs

**Example:** 30-minute interview with 2 assessors = â‚¬75

**Ivan's philosophy:**
> "This is just good practice, good hygiene, financial hygiene even for a startup."

**Why it matters:**
1. **Unit economics visibility** - Understand cost to acquire candidate
2. **Resource allocation** - Who's spending most time? Where?
3. **Monetization validation** - If cost â‚¬X per candidate, must charge â‚¬Y to partners/repreneurs
4. **Process optimization** - Identify expensive steps, automate or streamline
5. **Scalability planning** - At 100 candidates/year, total cost = ?

**Lesson learned:**
Track time and cost systematically, even without perfect tools. Helps inform pricing, resource decisions, and ROI analysis.

**Application to CRM model (future):**
Should track similar metrics but for different activities:
- Cost per repreneur (acquisition + service delivery)
- Cost per service delivered (coaching session, partner intro, deal flow)
- Revenue per repreneur (commissions + direct fees)
- Profit per repreneur (Revenue - Cost)
- LTV/CAC ratio (key SaaS metric applicable to Re-New)

**See:** [cost-tracking-methodology.md](../../07_ARCHIVE/ai-recruitment-platform/cost-tracking-methodology.md)

---

### Lesson 4: Personalization at Scale Builds Credibility

**Platform capability:** AI-generated personalized emails

**How it worked:**
- After CV screening, platform auto-generated email to candidate
- Email included 2-3 "connection points" between candidate background and Re-New mission
- Tailored language (not generic "we received your CV")
- Signed by Bertrand (with approval workflow)
- Sent 30-60 minutes after CV submission

**Example (paraphrased):**
> "Thank you for submitting your CV. After reviewing your profile, we identified several connections between your background and Re-New's mission: [Connection 1], [Connection 2], [Connection 3]. We'd like to learn more about where you are in your acquisition journey. Please complete this short questionnaire."

**Value proposition:**
- Demonstrates Re-New actually reviewed CV (not auto-reply)
- Makes candidate feel seen and understood
- Builds credibility before asking for more information (Stage 2 questionnaire)
- Higher response rates vs. generic acknowledgment

**Lesson learned:**
Personalization at scale is technically feasible and valuable for relationship businesses. But balance automation with authenticity.

**Application to future:**
- CRM email templates could include personalization variables (sector, stage, specific challenges)
- Partner introduction emails: Reference specific repreneur profile elements
- Automate structure, personalize content
- **Caution:** Over-automation risks feeling robotic; find balance

---

### Lesson 5: Structured Scoring Forces Strategic Clarity

**Platform feature:** 8 AI bots analyzing candidates across specific dimensions

**Dimensions scored (see [cv-extraction-fields.md](../../07_ARCHIVE/ai-recruitment-platform/cv-extraction-fields.md)):**
1. Distressed company management experience
2. Number of companies in career (4+ threshold)
3. C-suite experience (CEO, CFO, etc.)
4. Leadership role depth (0-8+ scale)
5. Team size managed (50+ people threshold)
6. Acquisition experience (M&A involvement)
7. Professional years of experience (10-20 sweet spot)
8. Training & development focus

**Why structured scoring mattered:**
- Forced Re-New to define "what makes a strong repreneur candidate?"
- Made criteria explicit and debatable (not gut feel)
- Enabled consistency across multiple reviewers
- Created data for calibration (does score predict interview performance?)

**Lesson learned:**
Even if AI doesn't do the scoring, having explicit rubric helps humans decide consistently. The exercise of defining criteria is valuable independent of automation.

**Campaign #2 retained this:**
Scoring framework simplified but retained structured approach:
- Professional Background (25%)
- Acquisition Readiness (30%)
- Business Acumen (25%)
- Personal Fit (20%)

**Application to future:**
- Partner evaluation rubric (which M&A firms to prioritize?)
- Deal quality scoring (which deals to share with repreneurs?)
- Service tier matching (which repreneurs fit which offers?)

**Meta-insight:** Defining evaluation criteria IS strategic work, not just operational task.

---

### Lesson 6: Tools Shape Thinking - Choose Carefully

**Platform design:** Built as ATS (Applicant Tracking System) with stages, scoring, pass/fail logic

**Unintended consequence:**
Platform reinforced selection mindset: "Which candidates pass? Which fail?"

**What this obscured:**
- Re-New isn't selecting employees (no job to offer)
- Re-New is managing entrepreneurs (ongoing relationships)
- Success isn't binary (pass/fail) but continuous (value delivered over time)

**Campaign #2 realization:**
> "ATS is dead, CRM is the future" - Amelie

**Why it matters:**
- Tool architecture encodes assumptions about the business
- ATS assumes funnel with endpoints (hired/rejected)
- CRM assumes lifecycle with stages (early/advanced/churned)
- Using wrong tool perpetuates wrong mental model

**Lesson learned:**
Technology choices have strategic implications. The structure of your tools influences how team thinks about the business. Choose tools that align with business model, not just technical requirements.

**Application to future:**
- CRM chosen because it encodes relationship management model
- Future platform decision: Marketplace (transactional) vs. Concierge (relational) shapes architecture
- Be intentional about what workflows tools enable/discourage

---

### Lesson 7: Validate Demand Before Building Tools

**What Ivan built:** Comprehensive platform with AI, dashboards, cost tracking, email automation

**Team reaction:** "Super cool," "amazing work"

**Business outcome:** Not deployed

**Why the mismatch:**
Platform solved problem Re-New no longer had (high-volume screening). Built before validating that automation was critical path to value.

**Ivan's retrospective framing (from other meetings):**
> "Take 5-10 guys, package an offer, sell it to them. If someone buys, THEN build the dashboard."

**What should have happened (hypothetically):**
1. Run Campaign #1 manually, track pain points
2. Identify: "Screening 60 CVs takes 10 hours, bottleneck"
3. Test: Use AI for pre-scoring on next 20 CVs manually
4. Measure: Did AI save time? Maintain quality?
5. Build: Only if ROI proven and problem persists

**Lesson learned:**
Even internal tools should be validated before building. Ask:
- Is this the bottleneck?
- Will solving this unlock revenue/growth?
- Can we test with minimal/manual version first?

**Application to future (already learned):**
Team now applies this lens consistently:
- CRM: Testing Airtable before custom build
- Matching: Manual spreadsheet before algorithm
- Platform: Email partners before building portal

**This is evidence of learning in action.**

---

### Lesson 8: Human Override is Non-Negotiable (for Re-New's Model)

**Platform design choice:** All AI scores advisory, human can override any field

**Why this mattered:**
Entrepreneurship support is inherently bespoke. Cookie-cutter evaluation misses:
- Non-traditional career paths
- Unique value propositions (network, expertise, passion)
- Contextual factors (family situation, financial backing, risk tolerance)
- Intangible qualities (coachability, resilience, cultural fit)

**Platform philosophy:**
AI assists (faster initial review), human decides (final judgment)

**Campaign #2 validation:**
Even with simplified scoring, all decisions are human-made:
- AI pre-scoring guides prioritization (A/B/C list)
- Interviews are conversations, not checklists
- Final selection is qualitative judgment by Bertrand/Amelie

**Lesson learned:**
For relationship-based businesses (coaching, advisory, curation), AI can augment but not replace human judgment. Build systems that enhance decision-making, not automate it.

**Counterexample where full automation works:**
- High-volume transactional businesses (loan approvals, fraud detection)
- Standardized products (commodity matching)
- Re-New is neither

**Application to future:**
- Partner matching: AI can suggest, Bertrand approves
- Deal curation: AI can filter, human selects what to share
- Service tier assignment: Algorithm recommends, relationship manager decides

---

### Lesson 9: Audit Trails Have Long-Term Value

**Platform feature:** Full assessment history stored

**Why Ivan included this:**
> "You will never know if one day you want to take back an interview and get some piece of information that could be useful for one of our partners... it's good to have track of everything."

**Use case (hypothetical):**
1. Candidate interviewed 6 months ago, not selected
2. New partner asks for candidates in specific niche sector
3. Search old interviews, find candidate with that expertise
4. Reactivate relationship, make introduction
5. Assessment investment pays off months later

**Campaign #2 implementation:**
Bertrand records all interviews in Notion with transcripts. Archive exists but underutilized.

**Ivan's suggestion (from other meeting):**
> "With recent technologies I could extract so many hidden layers [from interview transcripts] at scale."

**Lesson learned:**
Systematic record-keeping creates data asset with compounding value. But records only valuable if:
- Searchable (structured data or AI-powered semantic search)
- Analyzed (patterns extracted, not just stored)
- Acted upon (insights inform decisions)

**Application to future:**
- CRM should capture interaction history automatically
- Interview transcripts: Mine for patterns (what questions reveal most? common objections? success indicators?)
- Partner feedback: Track which intros succeed, optimize matching
- Service delivery notes: Build knowledge base of what works

---

### Lesson 10: Conversion Optimization â‰  Business Model Validation

**Platform excellence:** Highly optimized for user experience

**Features that improved conversion:**
- Two-step data collection (low friction)
- Personalized emails (builds credibility)
- Fast turnaround (30-60 min response)
- Clear process visualization (candidates knew where they stood)
- Automated follow-ups

**Problem:**
Optimizing wrong thing. High conversion to Stage 2 questionnaire didn't matter if Re-New's business model was wrong (selection vs. relationship management).

**Analogy:**
- Perfecting checkout flow for product no one wants to buy
- Optimizing lead gen for service you can't monetize
- Building fast horse when you need a car

**Lesson learned:**
User experience and conversion optimization are valuable AFTER validating core value proposition. Don't optimize funnel before validating destination.

**Application to future:**
Sequence matters:
1. **First:** Validate willingness-to-pay (will repreneurs/partners buy?)
2. **Second:** Validate offer-market fit (which services at what price?)
3. **Third:** Optimize delivery (CRM, automation, conversion funnels)

Don't reverse this order.

---

## Cross-Cutting Insights: AI Platform + Campaign #2

### What's Consistent Across Both Experiences

**1. Lead de Cadrage emerged as critical in both:**
- AI Platform: Stage 2 questionnaire captured project details
- Campaign #2: Lead de Cadrage became "magic key"
- **Learning:** Structured project profiling is Re-New's core data asset

**2. Cost tracking philosophy validated:**
- AI Platform: Built cost-per-assessment tracking
- Campaign #2: Realized need for cost-per-repreneur tracking
- **Learning:** Unit economics matter, should be measured systematically

**3. Personalization creates differentiation:**
- AI Platform: Auto-generated connection points in emails
- Campaign #2: Consultative interviews (vs. generic HR screening)
- **Learning:** Re-New's value is bespoke attention, not commodity process

**4. Tools must match business model:**
- AI Platform: ATS architecture reinforced wrong model
- Campaign #2: Shift to CRM aligned with relationship business
- **Learning:** Strategic clarity first, tooling second

---

### What Changed Between AI Platform and Campaign #2

| Dimension | AI Platform (Old) | Campaign #2 (New) |
|-----------|-------------------|-------------------|
| **Problem framed** | High-volume screening | Relationship depth at scale |
| **Bottleneck assumed** | CV processing speed | Service delivery bandwidth |
| **Success metric** | Cost per candidate screened | Revenue per repreneur served |
| **Business model** | Selection (filter many to few) | Ecosystem (support many at different stages) |
| **Primary tool** | ATS (applicant tracking) | CRM (customer relationship) |
| **Automation role** | Replace human screening | Augment human relationship management |
| **Core asset** | Scoring algorithm | Lead de Cadrage + Bertrand's judgment |

**Meta-learning:**
This table shows evolution of strategic understanding. AI Platform represented Re-New's early hypothesis. Campaign #2 revealed actual business model. Both were necessary steps in learning process.

---

### The Platform That Taught by Not Being Used

**Paradox:** AI Platform's greatest value wasn't its functionality, but the learning it enabled.

**What building (and not deploying) the platform taught:**
1. Re-New's real bottleneck isn't screening volume
2. Relationship management, not selection, is core competency
3. Tools are expensive and should be validated before building
4. Strategic clarity > technical sophistication
5. Business model must be proven before optimizing operations

**Ivan's (implicit) gift:**
Platform became teaching tool. Team's reaction to completed-but-unused platform forced confrontation with strategic questions:
- What business are we actually in?
- What problem are we really solving?
- Who are our customers?
- What should we build vs. buy vs. do manually?

**Lesson learned:**
Sometimes the most valuable product is the one that clarifies you're building the wrong product.

**Application to future:**
Treat every build/don't-build decision as hypothesis test. Be willing to abandon even "super cool" solutions if they don't serve validated business model.

---

## Change History

**v1.0 - November 1, 2024**
- Created initial version
- Source: Campaign #2 Debrief + Meeting Oct 24, 2024

**v1.1 - October 25, 2025**
- Added: AI Recruitment Platform Learnings section
- Source: AI Platform demonstration meeting (Oct 2024)
- Cross-referenced with archived platform documentation

**Next review:** After dual monetization experiment results (May 2025)

---

## Related Documents

**Decision Logs:**
- [DEC-20241101: ATS ï¿½ CRM](../../03_DECISION_LOG/2024-Q4/DEC-20241101-ats-to-crm-operational-pivot.md)
- [DEC-20241101: Dual Monetization](../../03_DECISION_LOG/2024-Q4/DEC-20241101-dual-monetization-testing.md)
- [DEC-20241101: Lead de Cadrage Mandatory](../../03_DECISION_LOG/2024-Q4/DEC-20241101-lead-de-cadrage-mandatory.md)

**CANONICAL:**
- [Business Model](../strategy/business-model.md)
- [Validation Assumptions](../strategy/validation-assumptions.md)
- [Stakeholder Map](stakeholder-map.md)
